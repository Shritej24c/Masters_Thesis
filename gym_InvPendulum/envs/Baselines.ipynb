{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baselines.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shritej24c/Q_Pendulum/blob/master/gym_InvPendulum/envs/Baselines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdEs94piif1z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 15657
        },
        "outputId": "10357941-e96b-4b17-ce77-aa48d0de24e5"
      },
      "source": [
        "import gym\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines.ddpg.policies import MlpPolicy\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines import PPO2, DQN, DDPG\n",
        "from stable_baselines.ddpg.noise import OrnsteinUhlenbeckActionNoise, ActionNoise, AdaptiveParamNoiseSpec, NormalActionNoise\n",
        "\n",
        "from Inv_pendulum import InvPendulumEnv\n",
        "\n",
        "Testenv = 'Test_Inv_pendulum-v0'\n",
        "Env = 'Inverted_Pendulum-v0'\n",
        "Og_Env = 'Pendulum-v0'\n",
        "\n",
        "env = gym.make(Env)\n",
        "\n",
        "env = DummyVecEnv([lambda: env])  # The algorithms require a vectorized environment to run\n",
        "\n",
        "# the noise objects for DDPG\n",
        "n_actions = env.action_space.shape[-1]\n",
        "param_noise = None\n",
        "action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=float(0.5) * np.ones(n_actions))\n",
        "\n",
        "model = DDPG(MlpPolicy, env, verbose=1, param_noise=param_noise, action_noise=action_noise)\n",
        "model.learn(total_timesteps=400000)\n",
        "\n",
        "model.save(\"ddpg_nd_nf\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.236   |\n",
            "| reference_Q_std        | 0.213    |\n",
            "| reference_action_mean  | 0.152    |\n",
            "| reference_action_std   | 0.0568   |\n",
            "| reference_actor_Q_mean | -0.219   |\n",
            "| reference_actor_Q_std  | 0.202    |\n",
            "| rollout/Q_mean         | -0.134   |\n",
            "| rollout/actions_mean   | -0.0586  |\n",
            "| rollout/actions_std    | 0.477    |\n",
            "| rollout/episode_steps  | 117      |\n",
            "| rollout/episodes       | 85       |\n",
            "| rollout/return         | -5.69    |\n",
            "| rollout/return_history | -5.69    |\n",
            "| total/duration         | 26.6     |\n",
            "| total/episodes         | 85       |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 9998     |\n",
            "| total/steps_per_second | 376      |\n",
            "| train/loss_actor       | 0.251    |\n",
            "| train/loss_critic      | 0.0025   |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.359   |\n",
            "| reference_Q_std        | 0.358    |\n",
            "| reference_action_mean  | 0.609    |\n",
            "| reference_action_std   | 0.0964   |\n",
            "| reference_actor_Q_mean | -0.356   |\n",
            "| reference_actor_Q_std  | 0.364    |\n",
            "| rollout/Q_mean         | -0.214   |\n",
            "| rollout/actions_mean   | 0.0683   |\n",
            "| rollout/actions_std    | 0.439    |\n",
            "| rollout/episode_steps  | 122      |\n",
            "| rollout/episodes       | 163      |\n",
            "| rollout/return         | -5.76    |\n",
            "| rollout/return_history | -5.76    |\n",
            "| total/duration         | 53.6     |\n",
            "| total/episodes         | 163      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 19998    |\n",
            "| total/steps_per_second | 373      |\n",
            "| train/loss_actor       | 0.321    |\n",
            "| train/loss_critic      | 0.000162 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.547   |\n",
            "| reference_Q_std        | 0.493    |\n",
            "| reference_action_mean  | -0.769   |\n",
            "| reference_action_std   | 0.0655   |\n",
            "| reference_actor_Q_mean | -0.54    |\n",
            "| reference_actor_Q_std  | 0.495    |\n",
            "| rollout/Q_mean         | -0.292   |\n",
            "| rollout/actions_mean   | 0.0558   |\n",
            "| rollout/actions_std    | 0.442    |\n",
            "| rollout/episode_steps  | 123      |\n",
            "| rollout/episodes       | 244      |\n",
            "| rollout/return         | -5.75    |\n",
            "| rollout/return_history | -5.84    |\n",
            "| total/duration         | 80.7     |\n",
            "| total/episodes         | 244      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 29998    |\n",
            "| total/steps_per_second | 372      |\n",
            "| train/loss_actor       | 0.79     |\n",
            "| train/loss_critic      | 0.00363  |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.723   |\n",
            "| reference_Q_std        | 0.613    |\n",
            "| reference_action_mean  | 0.935    |\n",
            "| reference_action_std   | 0.0612   |\n",
            "| reference_actor_Q_mean | -0.715   |\n",
            "| reference_actor_Q_std  | 0.607    |\n",
            "| rollout/Q_mean         | -0.391   |\n",
            "| rollout/actions_mean   | -0.0448  |\n",
            "| rollout/actions_std    | 0.475    |\n",
            "| rollout/episode_steps  | 116      |\n",
            "| rollout/episodes       | 345      |\n",
            "| rollout/return         | -5.72    |\n",
            "| rollout/return_history | -5.62    |\n",
            "| total/duration         | 108      |\n",
            "| total/episodes         | 345      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 39998    |\n",
            "| total/steps_per_second | 372      |\n",
            "| train/loss_actor       | 0.746    |\n",
            "| train/loss_critic      | 0.000313 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.978   |\n",
            "| reference_Q_std        | 0.754    |\n",
            "| reference_action_mean  | 0.937    |\n",
            "| reference_action_std   | 0.135    |\n",
            "| reference_actor_Q_mean | -1.03    |\n",
            "| reference_actor_Q_std  | 0.781    |\n",
            "| rollout/Q_mean         | -0.365   |\n",
            "| rollout/actions_mean   | -0.0649  |\n",
            "| rollout/actions_std    | 0.449    |\n",
            "| rollout/episode_steps  | 129      |\n",
            "| rollout/episodes       | 385      |\n",
            "| rollout/return         | -5.93    |\n",
            "| rollout/return_history | -6.55    |\n",
            "| total/duration         | 135      |\n",
            "| total/episodes         | 385      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 49998    |\n",
            "| total/steps_per_second | 370      |\n",
            "| train/loss_actor       | 0.377    |\n",
            "| train/loss_critic      | 0.00324  |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.941   |\n",
            "| reference_Q_std        | 0.786    |\n",
            "| reference_action_mean  | 0.995    |\n",
            "| reference_action_std   | 0.0108   |\n",
            "| reference_actor_Q_mean | -0.819   |\n",
            "| reference_actor_Q_std  | 0.684    |\n",
            "| rollout/Q_mean         | -0.354   |\n",
            "| rollout/actions_mean   | -0.0518  |\n",
            "| rollout/actions_std    | 0.43     |\n",
            "| rollout/episode_steps  | 141      |\n",
            "| rollout/episodes       | 425      |\n",
            "| rollout/return         | -6.08    |\n",
            "| rollout/return_history | -7.35    |\n",
            "| total/duration         | 162      |\n",
            "| total/episodes         | 425      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 59998    |\n",
            "| total/steps_per_second | 371      |\n",
            "| train/loss_actor       | 0.872    |\n",
            "| train/loss_critic      | 0.00623  |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -1.09    |\n",
            "| reference_Q_std        | 0.811    |\n",
            "| reference_action_mean  | 0.98     |\n",
            "| reference_action_std   | 0.0498   |\n",
            "| reference_actor_Q_mean | -1       |\n",
            "| reference_actor_Q_std  | 0.759    |\n",
            "| rollout/Q_mean         | -0.361   |\n",
            "| rollout/actions_mean   | -0.0602  |\n",
            "| rollout/actions_std    | 0.414    |\n",
            "| rollout/episode_steps  | 147      |\n",
            "| rollout/episodes       | 476      |\n",
            "| rollout/return         | -6.12    |\n",
            "| rollout/return_history | -7.12    |\n",
            "| total/duration         | 189      |\n",
            "| total/episodes         | 476      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 69998    |\n",
            "| total/steps_per_second | 371      |\n",
            "| train/loss_actor       | 0.133    |\n",
            "| train/loss_critic      | 0.000133 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.931   |\n",
            "| reference_Q_std        | 0.639    |\n",
            "| reference_action_mean  | 0.826    |\n",
            "| reference_action_std   | 0.447    |\n",
            "| reference_actor_Q_mean | -0.854   |\n",
            "| reference_actor_Q_std  | 0.59     |\n",
            "| rollout/Q_mean         | -0.36    |\n",
            "| rollout/actions_mean   | -0.045   |\n",
            "| rollout/actions_std    | 0.406    |\n",
            "| rollout/episode_steps  | 155      |\n",
            "| rollout/episodes       | 515      |\n",
            "| rollout/return         | -6.15    |\n",
            "| rollout/return_history | -6.51    |\n",
            "| total/duration         | 216      |\n",
            "| total/episodes         | 515      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 79998    |\n",
            "| total/steps_per_second | 370      |\n",
            "| train/loss_actor       | 0.387    |\n",
            "| train/loss_critic      | 0.000196 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.873   |\n",
            "| reference_Q_std        | 0.571    |\n",
            "| reference_action_mean  | 0.991    |\n",
            "| reference_action_std   | 0.0247   |\n",
            "| reference_actor_Q_mean | -0.786   |\n",
            "| reference_actor_Q_std  | 0.544    |\n",
            "| rollout/Q_mean         | -0.338   |\n",
            "| rollout/actions_mean   | -0.0295  |\n",
            "| rollout/actions_std    | 0.396    |\n",
            "| rollout/episode_steps  | 164      |\n",
            "| rollout/episodes       | 547      |\n",
            "| rollout/return         | -6.25    |\n",
            "| rollout/return_history | -6.88    |\n",
            "| total/duration         | 244      |\n",
            "| total/episodes         | 547      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 89998    |\n",
            "| total/steps_per_second | 369      |\n",
            "| train/loss_actor       | -0.121   |\n",
            "| train/loss_critic      | 0.000913 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.737   |\n",
            "| reference_Q_std        | 0.562    |\n",
            "| reference_action_mean  | 1        |\n",
            "| reference_action_std   | 0.00122  |\n",
            "| reference_actor_Q_mean | -0.653   |\n",
            "| reference_actor_Q_std  | 0.527    |\n",
            "| rollout/Q_mean         | -0.314   |\n",
            "| rollout/actions_mean   | -0.0261  |\n",
            "| rollout/actions_std    | 0.39     |\n",
            "| rollout/episode_steps  | 172      |\n",
            "| rollout/episodes       | 579      |\n",
            "| rollout/return         | -6.36    |\n",
            "| rollout/return_history | -7.52    |\n",
            "| total/duration         | 271      |\n",
            "| total/episodes         | 579      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 99998    |\n",
            "| total/steps_per_second | 369      |\n",
            "| train/loss_actor       | -0.0409  |\n",
            "| train/loss_critic      | 3.27e-05 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.723   |\n",
            "| reference_Q_std        | 0.563    |\n",
            "| reference_action_mean  | 1        |\n",
            "| reference_action_std   | 0.000644 |\n",
            "| reference_actor_Q_mean | -0.739   |\n",
            "| reference_actor_Q_std  | 0.55     |\n",
            "| rollout/Q_mean         | -0.28    |\n",
            "| rollout/actions_mean   | -0.0428  |\n",
            "| rollout/actions_std    | 0.391    |\n",
            "| rollout/episode_steps  | 182      |\n",
            "| rollout/episodes       | 602      |\n",
            "| rollout/return         | -6.54    |\n",
            "| rollout/return_history | -8.63    |\n",
            "| total/duration         | 298      |\n",
            "| total/episodes         | 602      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 109998   |\n",
            "| total/steps_per_second | 369      |\n",
            "| train/loss_actor       | -0.18    |\n",
            "| train/loss_critic      | 0.00034  |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.905   |\n",
            "| reference_Q_std        | 0.581    |\n",
            "| reference_action_mean  | 1        |\n",
            "| reference_action_std   | 6.93e-05 |\n",
            "| reference_actor_Q_mean | -0.911   |\n",
            "| reference_actor_Q_std  | 0.585    |\n",
            "| rollout/Q_mean         | -0.251   |\n",
            "| rollout/actions_mean   | -0.0524  |\n",
            "| rollout/actions_std    | 0.394    |\n",
            "| rollout/episode_steps  | 195      |\n",
            "| rollout/episodes       | 613      |\n",
            "| rollout/return         | -6.64    |\n",
            "| rollout/return_history | -9.19    |\n",
            "| total/duration         | 325      |\n",
            "| total/episodes         | 613      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 119998   |\n",
            "| total/steps_per_second | 369      |\n",
            "| train/loss_actor       | -0.0731  |\n",
            "| train/loss_critic      | 0.000172 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.954   |\n",
            "| reference_Q_std        | 0.58     |\n",
            "| reference_action_mean  | 1        |\n",
            "| reference_action_std   | 0.000127 |\n",
            "| reference_actor_Q_mean | -0.95    |\n",
            "| reference_actor_Q_std  | 0.59     |\n",
            "| rollout/Q_mean         | -0.229   |\n",
            "| rollout/actions_mean   | -0.061   |\n",
            "| rollout/actions_std    | 0.402    |\n",
            "| rollout/episode_steps  | 208      |\n",
            "| rollout/episodes       | 623      |\n",
            "| rollout/return         | -6.72    |\n",
            "| rollout/return_history | -9.61    |\n",
            "| total/duration         | 352      |\n",
            "| total/episodes         | 623      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 129998   |\n",
            "| total/steps_per_second | 370      |\n",
            "| train/loss_actor       | -0.113   |\n",
            "| train/loss_critic      | 1.14e-05 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.992   |\n",
            "| reference_Q_std        | 0.617    |\n",
            "| reference_action_mean  | 1        |\n",
            "| reference_action_std   | 8.02e-05 |\n",
            "| reference_actor_Q_mean | -0.905   |\n",
            "| reference_actor_Q_std  | 0.582    |\n",
            "| rollout/Q_mean         | -0.211   |\n",
            "| rollout/actions_mean   | -0.0617  |\n",
            "| rollout/actions_std    | 0.414    |\n",
            "| rollout/episode_steps  | 221      |\n",
            "| rollout/episodes       | 634      |\n",
            "| rollout/return         | -6.74    |\n",
            "| rollout/return_history | -9.51    |\n",
            "| total/duration         | 379      |\n",
            "| total/episodes         | 634      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 139998   |\n",
            "| total/steps_per_second | 370      |\n",
            "| train/loss_actor       | -0.0853  |\n",
            "| train/loss_critic      | 1.23e-06 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -1.07    |\n",
            "| reference_Q_std        | 0.654    |\n",
            "| reference_action_mean  | 1        |\n",
            "| reference_action_std   | 7.89e-05 |\n",
            "| reference_actor_Q_mean | -0.929   |\n",
            "| reference_actor_Q_std  | 0.592    |\n",
            "| rollout/Q_mean         | -0.198   |\n",
            "| rollout/actions_mean   | -0.0639  |\n",
            "| rollout/actions_std    | 0.423    |\n",
            "| rollout/episode_steps  | 231      |\n",
            "| rollout/episodes       | 645      |\n",
            "| rollout/return         | -6.75    |\n",
            "| rollout/return_history | -9.5     |\n",
            "| total/duration         | 406      |\n",
            "| total/episodes         | 645      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 149998   |\n",
            "| total/steps_per_second | 370      |\n",
            "| train/loss_actor       | -0.076   |\n",
            "| train/loss_critic      | 1.5e-06  |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -1       |\n",
            "| reference_Q_std        | 0.584    |\n",
            "| reference_action_mean  | 1        |\n",
            "| reference_action_std   | 2.71e-05 |\n",
            "| reference_actor_Q_mean | -0.88    |\n",
            "| reference_actor_Q_std  | 0.515    |\n",
            "| rollout/Q_mean         | -0.186   |\n",
            "| rollout/actions_mean   | -0.0682  |\n",
            "| rollout/actions_std    | 0.423    |\n",
            "| rollout/episode_steps  | 244      |\n",
            "| rollout/episodes       | 655      |\n",
            "| rollout/return         | -6.78    |\n",
            "| rollout/return_history | -9.44    |\n",
            "| total/duration         | 433      |\n",
            "| total/episodes         | 655      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 159998   |\n",
            "| total/steps_per_second | 370      |\n",
            "| train/loss_actor       | -0.0559  |\n",
            "| train/loss_critic      | 3.97e-05 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.756   |\n",
            "| reference_Q_std        | 0.472    |\n",
            "| reference_action_mean  | 1        |\n",
            "| reference_action_std   | 4.79e-05 |\n",
            "| reference_actor_Q_mean | -0.677   |\n",
            "| reference_actor_Q_std  | 0.454    |\n",
            "| rollout/Q_mean         | -0.175   |\n",
            "| rollout/actions_mean   | -0.0692  |\n",
            "| rollout/actions_std    | 0.419    |\n",
            "| rollout/episode_steps  | 255      |\n",
            "| rollout/episodes       | 665      |\n",
            "| rollout/return         | -6.8     |\n",
            "| rollout/return_history | -9.45    |\n",
            "| total/duration         | 460      |\n",
            "| total/episodes         | 665      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 169998   |\n",
            "| total/steps_per_second | 369      |\n",
            "| train/loss_actor       | -0.034   |\n",
            "| train/loss_critic      | 9.55e-06 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.717   |\n",
            "| reference_Q_std        | 0.485    |\n",
            "| reference_action_mean  | 0.689    |\n",
            "| reference_action_std   | 0.623    |\n",
            "| reference_actor_Q_mean | -0.608   |\n",
            "| reference_actor_Q_std  | 0.418    |\n",
            "| rollout/Q_mean         | -0.175   |\n",
            "| rollout/actions_mean   | -0.0591  |\n",
            "| rollout/actions_std    | 0.418    |\n",
            "| rollout/episode_steps  | 261      |\n",
            "| rollout/episodes       | 688      |\n",
            "| rollout/return         | -6.99    |\n",
            "| rollout/return_history | -10.3    |\n",
            "| total/duration         | 487      |\n",
            "| total/episodes         | 688      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 179998   |\n",
            "| total/steps_per_second | 369      |\n",
            "| train/loss_actor       | 0.202    |\n",
            "| train/loss_critic      | 3.54e-05 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.75    |\n",
            "| reference_Q_std        | 0.514    |\n",
            "| reference_action_mean  | 0.999    |\n",
            "| reference_action_std   | 0.0027   |\n",
            "| reference_actor_Q_mean | -0.747   |\n",
            "| reference_actor_Q_std  | 0.537    |\n",
            "| rollout/Q_mean         | -0.177   |\n",
            "| rollout/actions_mean   | -0.0518  |\n",
            "| rollout/actions_std    | 0.421    |\n",
            "| rollout/episode_steps  | 264      |\n",
            "| rollout/episodes       | 719      |\n",
            "| rollout/return         | -7.11    |\n",
            "| rollout/return_history | -9.71    |\n",
            "| total/duration         | 514      |\n",
            "| total/episodes         | 719      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 189998   |\n",
            "| total/steps_per_second | 370      |\n",
            "| train/loss_actor       | 0.703    |\n",
            "| train/loss_critic      | 0.00135  |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.77    |\n",
            "| reference_Q_std        | 0.566    |\n",
            "| reference_action_mean  | 0.998    |\n",
            "| reference_action_std   | 0.00735  |\n",
            "| reference_actor_Q_mean | -0.75    |\n",
            "| reference_actor_Q_std  | 0.587    |\n",
            "| rollout/Q_mean         | -0.177   |\n",
            "| rollout/actions_mean   | -0.0573  |\n",
            "| rollout/actions_std    | 0.424    |\n",
            "| rollout/episode_steps  | 270      |\n",
            "| rollout/episodes       | 737      |\n",
            "| rollout/return         | -7.17    |\n",
            "| rollout/return_history | -9.97    |\n",
            "| total/duration         | 542      |\n",
            "| total/episodes         | 737      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 199998   |\n",
            "| total/steps_per_second | 369      |\n",
            "| train/loss_actor       | 0.0425   |\n",
            "| train/loss_critic      | 2.45e-05 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.959   |\n",
            "| reference_Q_std        | 0.761    |\n",
            "| reference_action_mean  | 0.352    |\n",
            "| reference_action_std   | 0.868    |\n",
            "| reference_actor_Q_mean | -0.885   |\n",
            "| reference_actor_Q_std  | 0.756    |\n",
            "| rollout/Q_mean         | -0.186   |\n",
            "| rollout/actions_mean   | -0.0497  |\n",
            "| rollout/actions_std    | 0.422    |\n",
            "| rollout/episode_steps  | 266      |\n",
            "| rollout/episodes       | 788      |\n",
            "| rollout/return         | -7.15    |\n",
            "| rollout/return_history | -8.26    |\n",
            "| total/duration         | 569      |\n",
            "| total/episodes         | 788      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 209998   |\n",
            "| total/steps_per_second | 369      |\n",
            "| train/loss_actor       | -0.011   |\n",
            "| train/loss_critic      | 3.52e-05 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -1.22    |\n",
            "| reference_Q_std        | 0.981    |\n",
            "| reference_action_mean  | 0.999    |\n",
            "| reference_action_std   | 0.00122  |\n",
            "| reference_actor_Q_mean | -1.22    |\n",
            "| reference_actor_Q_std  | 0.997    |\n",
            "| rollout/Q_mean         | -0.19    |\n",
            "| rollout/actions_mean   | -0.0445  |\n",
            "| rollout/actions_std    | 0.423    |\n",
            "| rollout/episode_steps  | 266      |\n",
            "| rollout/episodes       | 824      |\n",
            "| rollout/return         | -7.13    |\n",
            "| rollout/return_history | -7       |\n",
            "| total/duration         | 596      |\n",
            "| total/episodes         | 824      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 219998   |\n",
            "| total/steps_per_second | 369      |\n",
            "| train/loss_actor       | 0.0246   |\n",
            "| train/loss_critic      | 6.67e-07 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -1.43    |\n",
            "| reference_Q_std        | 1.11     |\n",
            "| reference_action_mean  | 0.564    |\n",
            "| reference_action_std   | 0.711    |\n",
            "| reference_actor_Q_mean | -1.36    |\n",
            "| reference_actor_Q_std  | 1.09     |\n",
            "| rollout/Q_mean         | -0.191   |\n",
            "| rollout/actions_mean   | -0.0438  |\n",
            "| rollout/actions_std    | 0.426    |\n",
            "| rollout/episode_steps  | 273      |\n",
            "| rollout/episodes       | 842      |\n",
            "| rollout/return         | -7.14    |\n",
            "| rollout/return_history | -6.8     |\n",
            "| total/duration         | 623      |\n",
            "| total/episodes         | 842      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 229998   |\n",
            "| total/steps_per_second | 369      |\n",
            "| train/loss_actor       | 0.052    |\n",
            "| train/loss_critic      | 5.98e-05 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -1.3     |\n",
            "| reference_Q_std        | 1.02     |\n",
            "| reference_action_mean  | 1        |\n",
            "| reference_action_std   | 0.000179 |\n",
            "| reference_actor_Q_mean | -1.2     |\n",
            "| reference_actor_Q_std  | 0.962    |\n",
            "| rollout/Q_mean         | -0.193   |\n",
            "| rollout/actions_mean   | -0.038   |\n",
            "| rollout/actions_std    | 0.427    |\n",
            "| rollout/episode_steps  | 276      |\n",
            "| rollout/episodes       | 868      |\n",
            "| rollout/return         | -7.17    |\n",
            "| rollout/return_history | -7.12    |\n",
            "| total/duration         | 651      |\n",
            "| total/episodes         | 868      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 239998   |\n",
            "| total/steps_per_second | 369      |\n",
            "| train/loss_actor       | 0.62     |\n",
            "| train/loss_critic      | 0.000401 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -1.29    |\n",
            "| reference_Q_std        | 1        |\n",
            "| reference_action_mean  | 1        |\n",
            "| reference_action_std   | 7.54e-05 |\n",
            "| reference_actor_Q_mean | -1.23    |\n",
            "| reference_actor_Q_std  | 0.989    |\n",
            "| rollout/Q_mean         | -0.193   |\n",
            "| rollout/actions_mean   | -0.0373  |\n",
            "| rollout/actions_std    | 0.429    |\n",
            "| rollout/episode_steps  | 282      |\n",
            "| rollout/episodes       | 885      |\n",
            "| rollout/return         | -7.16    |\n",
            "| rollout/return_history | -7.25    |\n",
            "| total/duration         | 678      |\n",
            "| total/episodes         | 885      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 249998   |\n",
            "| total/steps_per_second | 369      |\n",
            "| train/loss_actor       | 0.0447   |\n",
            "| train/loss_critic      | 1.68e-06 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -1.09    |\n",
            "| reference_Q_std        | 0.887    |\n",
            "| reference_action_mean  | 1        |\n",
            "| reference_action_std   | 0.000869 |\n",
            "| reference_actor_Q_mean | -0.985   |\n",
            "| reference_actor_Q_std  | 0.833    |\n",
            "| rollout/Q_mean         | -0.191   |\n",
            "| rollout/actions_mean   | -0.0354  |\n",
            "| rollout/actions_std    | 0.432    |\n",
            "| rollout/episode_steps  | 288      |\n",
            "| rollout/episodes       | 898      |\n",
            "| rollout/return         | -7.15    |\n",
            "| rollout/return_history | -7.14    |\n",
            "| total/duration         | 705      |\n",
            "| total/episodes         | 898      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 259998   |\n",
            "| total/steps_per_second | 369      |\n",
            "| train/loss_actor       | 0.0505   |\n",
            "| train/loss_critic      | 1.16e-07 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -1.03    |\n",
            "| reference_Q_std        | 0.805    |\n",
            "| reference_action_mean  | 0.999    |\n",
            "| reference_action_std   | 0.00395  |\n",
            "| reference_actor_Q_mean | -1.02    |\n",
            "| reference_actor_Q_std  | 0.79     |\n",
            "| rollout/Q_mean         | -0.19    |\n",
            "| rollout/actions_mean   | -0.0276  |\n",
            "| rollout/actions_std    | 0.435    |\n",
            "| rollout/episode_steps  | 295      |\n",
            "| rollout/episodes       | 911      |\n",
            "| rollout/return         | -7.2     |\n",
            "| rollout/return_history | -7.76    |\n",
            "| total/duration         | 733      |\n",
            "| total/episodes         | 911      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 269998   |\n",
            "| total/steps_per_second | 368      |\n",
            "| train/loss_actor       | 0.0558   |\n",
            "| train/loss_critic      | 0.000368 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -1.14    |\n",
            "| reference_Q_std        | 0.883    |\n",
            "| reference_action_mean  | 1        |\n",
            "| reference_action_std   | 1.07e-05 |\n",
            "| reference_actor_Q_mean | -1.1     |\n",
            "| reference_actor_Q_std  | 0.869    |\n",
            "| rollout/Q_mean         | -0.191   |\n",
            "| rollout/actions_mean   | -0.0215  |\n",
            "| rollout/actions_std    | 0.436    |\n",
            "| rollout/episode_steps  | 303      |\n",
            "| rollout/episodes       | 923      |\n",
            "| rollout/return         | -7.27    |\n",
            "| rollout/return_history | -8.41    |\n",
            "| total/duration         | 761      |\n",
            "| total/episodes         | 923      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 279998   |\n",
            "| total/steps_per_second | 368      |\n",
            "| train/loss_actor       | 0.107    |\n",
            "| train/loss_critic      | 3.99e-08 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -1.18    |\n",
            "| reference_Q_std        | 0.89     |\n",
            "| reference_action_mean  | 1        |\n",
            "| reference_action_std   | 1.73e-07 |\n",
            "| reference_actor_Q_mean | -1.18    |\n",
            "| reference_actor_Q_std  | 0.909    |\n",
            "| rollout/Q_mean         | -0.191   |\n",
            "| rollout/actions_mean   | -0.0234  |\n",
            "| rollout/actions_std    | 0.441    |\n",
            "| rollout/episode_steps  | 310      |\n",
            "| rollout/episodes       | 934      |\n",
            "| rollout/return         | -7.27    |\n",
            "| rollout/return_history | -8.36    |\n",
            "| total/duration         | 789      |\n",
            "| total/episodes         | 934      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 289998   |\n",
            "| total/steps_per_second | 368      |\n",
            "| train/loss_actor       | 0.647    |\n",
            "| train/loss_critic      | 0.000331 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -1.29    |\n",
            "| reference_Q_std        | 0.927    |\n",
            "| reference_action_mean  | 1        |\n",
            "| reference_action_std   | 1.86e-05 |\n",
            "| reference_actor_Q_mean | -1.25    |\n",
            "| reference_actor_Q_std  | 0.919    |\n",
            "| rollout/Q_mean         | -0.191   |\n",
            "| rollout/actions_mean   | -0.0268  |\n",
            "| rollout/actions_std    | 0.443    |\n",
            "| rollout/episode_steps  | 318      |\n",
            "| rollout/episodes       | 942      |\n",
            "| rollout/return         | -7.29    |\n",
            "| rollout/return_history | -8.55    |\n",
            "| total/duration         | 816      |\n",
            "| total/episodes         | 942      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 299998   |\n",
            "| total/steps_per_second | 368      |\n",
            "| train/loss_actor       | 0.131    |\n",
            "| train/loss_critic      | 3.95e-05 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -1.08    |\n",
            "| reference_Q_std        | 0.793    |\n",
            "| reference_action_mean  | 1        |\n",
            "| reference_action_std   | 1.02e-05 |\n",
            "| reference_actor_Q_mean | -1.03    |\n",
            "| reference_actor_Q_std  | 0.736    |\n",
            "| rollout/Q_mean         | -0.192   |\n",
            "| rollout/actions_mean   | -0.0208  |\n",
            "| rollout/actions_std    | 0.447    |\n",
            "| rollout/episode_steps  | 324      |\n",
            "| rollout/episodes       | 952      |\n",
            "| rollout/return         | -7.32    |\n",
            "| rollout/return_history | -8.85    |\n",
            "| total/duration         | 844      |\n",
            "| total/episodes         | 952      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 309998   |\n",
            "| total/steps_per_second | 367      |\n",
            "| train/loss_actor       | 0.139    |\n",
            "| train/loss_critic      | 5.4e-06  |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.937   |\n",
            "| reference_Q_std        | 0.619    |\n",
            "| reference_action_mean  | 1        |\n",
            "| reference_action_std   | 2.9e-05  |\n",
            "| reference_actor_Q_mean | -1.02    |\n",
            "| reference_actor_Q_std  | 0.676    |\n",
            "| rollout/Q_mean         | -0.193   |\n",
            "| rollout/actions_mean   | -0.0154  |\n",
            "| rollout/actions_std    | 0.453    |\n",
            "| rollout/episode_steps  | 332      |\n",
            "| rollout/episodes       | 963      |\n",
            "| rollout/return         | -7.35    |\n",
            "| rollout/return_history | -9.14    |\n",
            "| total/duration         | 873      |\n",
            "| total/episodes         | 963      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 319998   |\n",
            "| total/steps_per_second | 366      |\n",
            "| train/loss_actor       | 0.853    |\n",
            "| train/loss_critic      | 0.00641  |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.994   |\n",
            "| reference_Q_std        | 0.635    |\n",
            "| reference_action_mean  | 1        |\n",
            "| reference_action_std   | 0.000129 |\n",
            "| reference_actor_Q_mean | -1.04    |\n",
            "| reference_actor_Q_std  | 0.666    |\n",
            "| rollout/Q_mean         | -0.195   |\n",
            "| rollout/actions_mean   | -0.00946 |\n",
            "| rollout/actions_std    | 0.459    |\n",
            "| rollout/episode_steps  | 339      |\n",
            "| rollout/episodes       | 971      |\n",
            "| rollout/return         | -7.4     |\n",
            "| rollout/return_history | -9.47    |\n",
            "| total/duration         | 901      |\n",
            "| total/episodes         | 971      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 329998   |\n",
            "| total/steps_per_second | 366      |\n",
            "| train/loss_actor       | 0.182    |\n",
            "| train/loss_critic      | 2.33e-07 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -1.32    |\n",
            "| reference_Q_std        | 0.788    |\n",
            "| reference_action_mean  | 1        |\n",
            "| reference_action_std   | 2.41e-08 |\n",
            "| reference_actor_Q_mean | -1.44    |\n",
            "| reference_actor_Q_std  | 0.868    |\n",
            "| rollout/Q_mean         | -0.197   |\n",
            "| rollout/actions_mean   | -0.00821 |\n",
            "| rollout/actions_std    | 0.467    |\n",
            "| rollout/episode_steps  | 346      |\n",
            "| rollout/episodes       | 980      |\n",
            "| rollout/return         | -7.42    |\n",
            "| rollout/return_history | -9.75    |\n",
            "| total/duration         | 929      |\n",
            "| total/episodes         | 980      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 339998   |\n",
            "| total/steps_per_second | 366      |\n",
            "| train/loss_actor       | 0.216    |\n",
            "| train/loss_critic      | 7.13e-05 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -1.55    |\n",
            "| reference_Q_std        | 0.843    |\n",
            "| reference_action_mean  | 1        |\n",
            "| reference_action_std   | 0        |\n",
            "| reference_actor_Q_mean | -1.49    |\n",
            "| reference_actor_Q_std  | 0.78     |\n",
            "| rollout/Q_mean         | -0.2     |\n",
            "| rollout/actions_mean   | -0.0127  |\n",
            "| rollout/actions_std    | 0.473    |\n",
            "| rollout/episode_steps  | 353      |\n",
            "| rollout/episodes       | 990      |\n",
            "| rollout/return         | -7.45    |\n",
            "| rollout/return_history | -10.1    |\n",
            "| total/duration         | 957      |\n",
            "| total/episodes         | 990      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 349998   |\n",
            "| total/steps_per_second | 366      |\n",
            "| train/loss_actor       | 0.241    |\n",
            "| train/loss_critic      | 1.39e-05 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -1.71    |\n",
            "| reference_Q_std        | 0.795    |\n",
            "| reference_action_mean  | 1        |\n",
            "| reference_action_std   | 0        |\n",
            "| reference_actor_Q_mean | -1.68    |\n",
            "| reference_actor_Q_std  | 0.768    |\n",
            "| rollout/Q_mean         | -0.203   |\n",
            "| rollout/actions_mean   | -0.017   |\n",
            "| rollout/actions_std    | 0.479    |\n",
            "| rollout/episode_steps  | 360      |\n",
            "| rollout/episodes       | 1e+03    |\n",
            "| rollout/return         | -7.49    |\n",
            "| rollout/return_history | -10.5    |\n",
            "| total/duration         | 984      |\n",
            "| total/episodes         | 1e+03    |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 359998   |\n",
            "| total/steps_per_second | 366      |\n",
            "| train/loss_actor       | 0.708    |\n",
            "| train/loss_critic      | 0.0198   |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -1.76    |\n",
            "| reference_Q_std        | 0.785    |\n",
            "| reference_action_mean  | 1        |\n",
            "| reference_action_std   | 1.04e-07 |\n",
            "| reference_actor_Q_mean | -1.75    |\n",
            "| reference_actor_Q_std  | 0.768    |\n",
            "| rollout/Q_mean         | -0.206   |\n",
            "| rollout/actions_mean   | -0.0211  |\n",
            "| rollout/actions_std    | 0.485    |\n",
            "| rollout/episode_steps  | 368      |\n",
            "| rollout/episodes       | 1.00e+03 |\n",
            "| rollout/return         | -7.53    |\n",
            "| rollout/return_history | -10.9    |\n",
            "| total/duration         | 1.01e+03 |\n",
            "| total/episodes         | 1.00e+03 |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 369998   |\n",
            "| total/steps_per_second | 366      |\n",
            "| train/loss_actor       | 0.239    |\n",
            "| train/loss_critic      | 1.76e-07 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -1.88    |\n",
            "| reference_Q_std        | 0.769    |\n",
            "| reference_action_mean  | 1        |\n",
            "| reference_action_std   | 0        |\n",
            "| reference_actor_Q_mean | -1.9     |\n",
            "| reference_actor_Q_std  | 0.761    |\n",
            "| rollout/Q_mean         | -0.21    |\n",
            "| rollout/actions_mean   | -0.0253  |\n",
            "| rollout/actions_std    | 0.49     |\n",
            "| rollout/episode_steps  | 372      |\n",
            "| rollout/episodes       | 1.02e+03 |\n",
            "| rollout/return         | -7.57    |\n",
            "| rollout/return_history | -10.5    |\n",
            "| total/duration         | 1.04e+03 |\n",
            "| total/episodes         | 1.02e+03 |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 379998   |\n",
            "| total/steps_per_second | 365      |\n",
            "| train/loss_actor       | 0.365    |\n",
            "| train/loss_critic      | 8.85e-05 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -1.82    |\n",
            "| reference_Q_std        | 0.838    |\n",
            "| reference_action_mean  | 1        |\n",
            "| reference_action_std   | 0        |\n",
            "| reference_actor_Q_mean | -1.83    |\n",
            "| reference_actor_Q_std  | 0.819    |\n",
            "| rollout/Q_mean         | -0.215   |\n",
            "| rollout/actions_mean   | -0.0313  |\n",
            "| rollout/actions_std    | 0.493    |\n",
            "| rollout/episode_steps  | 378      |\n",
            "| rollout/episodes       | 1.03e+03 |\n",
            "| rollout/return         | -7.67    |\n",
            "| rollout/return_history | -11.5    |\n",
            "| total/duration         | 1.07e+03 |\n",
            "| total/episodes         | 1.03e+03 |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 389998   |\n",
            "| total/steps_per_second | 365      |\n",
            "| train/loss_actor       | 0.36     |\n",
            "| train/loss_critic      | 6.83e-05 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -1.89    |\n",
            "| reference_Q_std        | 0.82     |\n",
            "| reference_action_mean  | 1        |\n",
            "| reference_action_std   | 0        |\n",
            "| reference_actor_Q_mean | -1.89    |\n",
            "| reference_actor_Q_std  | 0.811    |\n",
            "| rollout/Q_mean         | -0.222   |\n",
            "| rollout/actions_mean   | -0.0369  |\n",
            "| rollout/actions_std    | 0.497    |\n",
            "| rollout/episode_steps  | 382      |\n",
            "| rollout/episodes       | 1.04e+03 |\n",
            "| rollout/return         | -7.76    |\n",
            "| rollout/return_history | -12.1    |\n",
            "| total/duration         | 1.1e+03  |\n",
            "| total/episodes         | 1.04e+03 |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 399998   |\n",
            "| total/steps_per_second | 365      |\n",
            "| train/loss_actor       | 0.403    |\n",
            "| train/loss_critic      | 2.3e-05  |\n",
            "-------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9P1sqvRpwo0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "f41d7ebe-6df1-4295-82e9-0fef629f492a"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "import matplotlib.pyplot as plt\n",
        "import Inv_pendulum\n",
        "from stable_baselines import DDPG\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "\n",
        "\n",
        "metadata_ = {\n",
        "        'render.modes': ['human', 'rgb_array'],\n",
        "        'video.frames_per_second': 30\n",
        "    }\n",
        "\n",
        "\n",
        "Xnew = [np.array([[[0.29466096, 0.30317302]]])]\n",
        "\n",
        "\n",
        "X = np.array([0.29466096, 0.30317302])\n",
        "\n",
        "\n",
        "env_test = gym.make('Inverted_Pendulum-v0')\n",
        "env_ = DummyVecEnv([lambda: env_test])  # The algorithms require a vectorized environment to run\n",
        "\n",
        "obs_ = env_.reset()\n",
        "\n",
        "print(\"Xnew\", Xnew)\n",
        "print(\"X\",X)\n",
        "print(\"obs\", obs_)\n",
        "\n",
        "model_ = DDPG.load(\"ddpg_damp\")\n",
        "\n",
        "\n",
        "theta = []\n",
        "theta_dot = []\n",
        "actions = []\n",
        "\n",
        "\n",
        "def play(env, model, video_path, num_episodes, timesteps, metadata):\n",
        "    for i_episodes in range(num_episodes):\n",
        "        video_recorder = VideoRecorder(\n",
        "            env=env, path=video_path, metadata=metadata, enabled=video_path is not None)\n",
        "        obs = env.reset()\n",
        "        print(obs)\n",
        "        for t in range(timesteps):\n",
        "            video_recorder.capture_frame()\n",
        "            action, states = model.predict(obs)\n",
        "            obs, rew, done, info = env.step(action)\n",
        "            env.render()\n",
        "            theta.append(obs[0][0])\n",
        "            theta_dot.append(obs[0][1])\n",
        "            actions.append(action[0][0])\n",
        "            if done:\n",
        "                print(\"Episode finished after {} timesteps\".format(t + 1))\n",
        "                num_episodes += 1\n",
        "                # save video of first episode\n",
        "                print(\"Saved video.\")\n",
        "                video_recorder.close()\n",
        "                video_recorder.enabled = False\n",
        "                break\n",
        "            elif t == timesteps-1:\n",
        "                print(\"The episode done with all the timesteps\")\n",
        "                print(\"Saved video\")\n",
        "                video_recorder.close()\n",
        "                video_recorder.enabled = False\n",
        "                break\n",
        "    env.close()\n",
        "    return theta\n",
        "\n",
        "\n",
        "plt.plot(np.array(play(env_, model_, \"DDPG_d.mp4\", 1, 1000, metadata_)))\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Xnew [array([[[0.29466096, 0.30317302]]])]\n",
            "X [0.29466096 0.30317302]\n",
            "obs [[-0.08198508  0.0438564 ]]\n",
            "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3739fee980ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DDPG_d.mp4\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-3739fee980ef>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(env, model, video_path, num_episodes, timesteps, metadata)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi_episodes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         video_recorder = VideoRecorder(\n\u001b[0;32m---> 42\u001b[0;31m             env=env, path=video_path, metadata=metadata, enabled=video_path is not None)\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, path, metadata, enabled, base_path)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mmodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'render.modes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_async\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'semantics.async'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DummyVecEnv' object has no attribute 'metadata'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YodHZF45uJh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"ddpg_damp.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NukkVpdbZ7Bi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff70abbb-f5c9-45b5-cf67-10280d36785c"
      },
      "source": [
        "import stable_baselines\n",
        "stable_baselines.__version__"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL6x_59pZ_1V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fc9936d7-47af-4f26-c066-d9a1db703396"
      },
      "source": [
        "pip install stable_baselines >= 2.6.0"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31m  ERROR: Could not find a version that satisfies the requirement 2.6.0 (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for 2.6.0\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEoxi9h5agUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}