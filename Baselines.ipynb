{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baselines.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shritej24c/Q_Pendulum/blob/master/Baselines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdEs94piif1z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13875
        },
        "outputId": "10976660-9e89-4e76-fa10-bcc4aa47ed0d"
      },
      "source": [
        "import gym\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines.ddpg.policies import MlpPolicy\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines import PPO2, DQN, DDPG\n",
        "from stable_baselines.ddpg.noise import OrnsteinUhlenbeckActionNoise, ActionNoise, AdaptiveParamNoiseSpec, NormalActionNoise\n",
        "\n",
        "from Inv_pendulum import InvPendulumEnv\n",
        "\n",
        "Testenv = 'Test_Inv_pendulum-v0'\n",
        "Env = 'Inverted_Pendulum-v0'\n",
        "Og_Env = 'Pendulum-v0'\n",
        "\n",
        "env = gym.make(Env)\n",
        "\n",
        "env = DummyVecEnv([lambda: env])  # The algorithms require a vectorized environment to run\n",
        "\n",
        "# the noise objects for DDPG\n",
        "n_actions = env.action_space.shape[-1]\n",
        "param_noise = None\n",
        "action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=float(0.5) * np.ones(n_actions))\n",
        "\n",
        "model = DDPG(MlpPolicy, env, verbose=1, param_noise=param_noise, action_noise=action_noise)\n",
        "model.learn(total_timesteps=400000)\n",
        "\n",
        "model.save(\"ddpg_damp\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0619 07:05:07.844112 139768840390528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:85: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0619 07:05:07.846506 139768840390528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:94: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "W0619 07:05:07.918970 139768840390528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/stable_baselines/ddpg/ddpg.py:280: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0619 07:05:07.920819 139768840390528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0619 07:05:07.931117 139768840390528 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/input.py:26: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0619 07:05:07.949831 139768840390528 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/stable_baselines/ddpg/policies.py:132: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "W0619 07:05:08.218292 139768840390528 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/stable_baselines/ddpg/policies.py:134: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0619 07:05:08.221104 139768840390528 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0619 07:05:08.969975 139768840390528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/stable_baselines/ddpg/ddpg.py:357: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0619 07:05:08.979945 139768840390528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/stable_baselines/ddpg/ddpg.py:358: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "W0619 07:05:09.004466 139768840390528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:496: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "W0619 07:05:09.093729 139768840390528 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0619 07:05:09.680554 139768840390528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/stable_baselines/ddpg/ddpg.py:387: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.278   |\n",
            "| reference_Q_std        | 0.248    |\n",
            "| reference_action_mean  | -0.183   |\n",
            "| reference_action_std   | 0.513    |\n",
            "| reference_actor_Q_mean | -0.259   |\n",
            "| reference_actor_Q_std  | 0.224    |\n",
            "| rollout/Q_mean         | -0.132   |\n",
            "| rollout/actions_mean   | -0.0308  |\n",
            "| rollout/actions_std    | 0.382    |\n",
            "| rollout/episode_steps  | 121      |\n",
            "| rollout/episodes       | 82       |\n",
            "| rollout/return         | -5.85    |\n",
            "| rollout/return_history | -5.85    |\n",
            "| total/duration         | 26.4     |\n",
            "| total/episodes         | 82       |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 9998     |\n",
            "| total/steps_per_second | 379      |\n",
            "| train/loss_actor       | 0.236    |\n",
            "| train/loss_critic      | 0.00193  |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.516   |\n",
            "| reference_Q_std        | 0.422    |\n",
            "| reference_action_mean  | 0.308    |\n",
            "| reference_action_std   | 0.695    |\n",
            "| reference_actor_Q_mean | -0.473   |\n",
            "| reference_actor_Q_std  | 0.435    |\n",
            "| rollout/Q_mean         | -0.267   |\n",
            "| rollout/actions_mean   | 0.00272  |\n",
            "| rollout/actions_std    | 0.508    |\n",
            "| rollout/episode_steps  | 110      |\n",
            "| rollout/episodes       | 181      |\n",
            "| rollout/return         | -6.18    |\n",
            "| rollout/return_history | -6.44    |\n",
            "| total/duration         | 53.6     |\n",
            "| total/episodes         | 181      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 19998    |\n",
            "| total/steps_per_second | 373      |\n",
            "| train/loss_actor       | 0.536    |\n",
            "| train/loss_critic      | 0.00235  |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.689   |\n",
            "| reference_Q_std        | 0.597    |\n",
            "| reference_action_mean  | 0.358    |\n",
            "| reference_action_std   | 0.704    |\n",
            "| reference_actor_Q_mean | -0.649   |\n",
            "| reference_actor_Q_std  | 0.622    |\n",
            "| rollout/Q_mean         | -0.409   |\n",
            "| rollout/actions_mean   | 0.0606   |\n",
            "| rollout/actions_std    | 0.574    |\n",
            "| rollout/episode_steps  | 102      |\n",
            "| rollout/episodes       | 294      |\n",
            "| rollout/return         | -6.13    |\n",
            "| rollout/return_history | -6.04    |\n",
            "| total/duration         | 80.3     |\n",
            "| total/episodes         | 294      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 29998    |\n",
            "| total/steps_per_second | 374      |\n",
            "| train/loss_actor       | 0.995    |\n",
            "| train/loss_critic      | 0.0019   |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.903   |\n",
            "| reference_Q_std        | 0.805    |\n",
            "| reference_action_mean  | -0.959   |\n",
            "| reference_action_std   | 0.0202   |\n",
            "| reference_actor_Q_mean | -0.831   |\n",
            "| reference_actor_Q_std  | 0.763    |\n",
            "| rollout/Q_mean         | -0.478   |\n",
            "| rollout/actions_mean   | 0.0651   |\n",
            "| rollout/actions_std    | 0.551    |\n",
            "| rollout/episode_steps  | 103      |\n",
            "| rollout/episodes       | 387      |\n",
            "| rollout/return         | -6.02    |\n",
            "| rollout/return_history | -5.7     |\n",
            "| total/duration         | 107      |\n",
            "| total/episodes         | 387      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 39998    |\n",
            "| total/steps_per_second | 375      |\n",
            "| train/loss_actor       | 0.666    |\n",
            "| train/loss_critic      | 0.00551  |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.97    |\n",
            "| reference_Q_std        | 0.831    |\n",
            "| reference_action_mean  | -0.49    |\n",
            "| reference_action_std   | 0.647    |\n",
            "| reference_actor_Q_mean | -0.926   |\n",
            "| reference_actor_Q_std  | 0.801    |\n",
            "| rollout/Q_mean         | -0.499   |\n",
            "| rollout/actions_mean   | -0.00279 |\n",
            "| rollout/actions_std    | 0.565    |\n",
            "| rollout/episode_steps  | 108      |\n",
            "| rollout/episodes       | 459      |\n",
            "| rollout/return         | -6       |\n",
            "| rollout/return_history | -5.92    |\n",
            "| total/duration         | 133      |\n",
            "| total/episodes         | 459      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 49998    |\n",
            "| total/steps_per_second | 375      |\n",
            "| train/loss_actor       | 0.28     |\n",
            "| train/loss_critic      | 4.66e-05 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.901   |\n",
            "| reference_Q_std        | 0.799    |\n",
            "| reference_action_mean  | -0.173   |\n",
            "| reference_action_std   | 0.853    |\n",
            "| reference_actor_Q_mean | -0.85    |\n",
            "| reference_actor_Q_std  | 0.768    |\n",
            "| rollout/Q_mean         | -0.474   |\n",
            "| rollout/actions_mean   | 0.0164   |\n",
            "| rollout/actions_std    | 0.533    |\n",
            "| rollout/episode_steps  | 122      |\n",
            "| rollout/episodes       | 490      |\n",
            "| rollout/return         | -6.21    |\n",
            "| rollout/return_history | -7       |\n",
            "| total/duration         | 161      |\n",
            "| total/episodes         | 490      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 59998    |\n",
            "| total/steps_per_second | 373      |\n",
            "| train/loss_actor       | 0.646    |\n",
            "| train/loss_critic      | 0.000452 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.832   |\n",
            "| reference_Q_std        | 0.791    |\n",
            "| reference_action_mean  | 0.135    |\n",
            "| reference_action_std   | 0.985    |\n",
            "| reference_actor_Q_mean | -0.752   |\n",
            "| reference_actor_Q_std  | 0.732    |\n",
            "| rollout/Q_mean         | -0.442   |\n",
            "| rollout/actions_mean   | 0.024    |\n",
            "| rollout/actions_std    | 0.506    |\n",
            "| rollout/episode_steps  | 137      |\n",
            "| rollout/episodes       | 510      |\n",
            "| rollout/return         | -6.38    |\n",
            "| rollout/return_history | -7.99    |\n",
            "| total/duration         | 188      |\n",
            "| total/episodes         | 510      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 69998    |\n",
            "| total/steps_per_second | 372      |\n",
            "| train/loss_actor       | 0.13     |\n",
            "| train/loss_critic      | 2.83e-06 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.757   |\n",
            "| reference_Q_std        | 0.714    |\n",
            "| reference_action_mean  | 0.119    |\n",
            "| reference_action_std   | 0.974    |\n",
            "| reference_actor_Q_mean | -0.742   |\n",
            "| reference_actor_Q_std  | 0.71     |\n",
            "| rollout/Q_mean         | -0.414   |\n",
            "| rollout/actions_mean   | 0.0227   |\n",
            "| rollout/actions_std    | 0.496    |\n",
            "| rollout/episode_steps  | 152      |\n",
            "| rollout/episodes       | 524      |\n",
            "| rollout/return         | -6.43    |\n",
            "| rollout/return_history | -8.42    |\n",
            "| total/duration         | 215      |\n",
            "| total/episodes         | 524      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 79998    |\n",
            "| total/steps_per_second | 373      |\n",
            "| train/loss_actor       | 0.104    |\n",
            "| train/loss_critic      | 1.77e-05 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.864   |\n",
            "| reference_Q_std        | 0.779    |\n",
            "| reference_action_mean  | -0.795   |\n",
            "| reference_action_std   | 0.568    |\n",
            "| reference_actor_Q_mean | -0.809   |\n",
            "| reference_actor_Q_std  | 0.729    |\n",
            "| rollout/Q_mean         | -0.406   |\n",
            "| rollout/actions_mean   | 0.0497   |\n",
            "| rollout/actions_std    | 0.487    |\n",
            "| rollout/episode_steps  | 161      |\n",
            "| rollout/episodes       | 560      |\n",
            "| rollout/return         | -6.63    |\n",
            "| rollout/return_history | -9.34    |\n",
            "| total/duration         | 241      |\n",
            "| total/episodes         | 560      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 89998    |\n",
            "| total/steps_per_second | 373      |\n",
            "| train/loss_actor       | 0.724    |\n",
            "| train/loss_critic      | 0.00464  |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.813   |\n",
            "| reference_Q_std        | 0.742    |\n",
            "| reference_action_mean  | -0.604   |\n",
            "| reference_action_std   | 0.741    |\n",
            "| reference_actor_Q_mean | -0.778   |\n",
            "| reference_actor_Q_std  | 0.711    |\n",
            "| rollout/Q_mean         | -0.409   |\n",
            "| rollout/actions_mean   | 0.0618   |\n",
            "| rollout/actions_std    | 0.479    |\n",
            "| rollout/episode_steps  | 165      |\n",
            "| rollout/episodes       | 606      |\n",
            "| rollout/return         | -6.63    |\n",
            "| rollout/return_history | -8.15    |\n",
            "| total/duration         | 268      |\n",
            "| total/episodes         | 606      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 99998    |\n",
            "| total/steps_per_second | 373      |\n",
            "| train/loss_actor       | 0.117    |\n",
            "| train/loss_critic      | 2.11e-05 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.699   |\n",
            "| reference_Q_std        | 0.646    |\n",
            "| reference_action_mean  | -0.247   |\n",
            "| reference_action_std   | 0.895    |\n",
            "| reference_actor_Q_mean | -0.646   |\n",
            "| reference_actor_Q_std  | 0.645    |\n",
            "| rollout/Q_mean         | -0.409   |\n",
            "| rollout/actions_mean   | 0.0719   |\n",
            "| rollout/actions_std    | 0.476    |\n",
            "| rollout/episode_steps  | 170      |\n",
            "| rollout/episodes       | 645      |\n",
            "| rollout/return         | -6.66    |\n",
            "| rollout/return_history | -6.93    |\n",
            "| total/duration         | 295      |\n",
            "| total/episodes         | 645      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 109998   |\n",
            "| total/steps_per_second | 373      |\n",
            "| train/loss_actor       | 0.192    |\n",
            "| train/loss_critic      | 5.4e-06  |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.649   |\n",
            "| reference_Q_std        | 0.627    |\n",
            "| reference_action_mean  | 0.0786   |\n",
            "| reference_action_std   | 0.967    |\n",
            "| reference_actor_Q_mean | -0.608   |\n",
            "| reference_actor_Q_std  | 0.629    |\n",
            "| rollout/Q_mean         | -0.401   |\n",
            "| rollout/actions_mean   | 0.0828   |\n",
            "| rollout/actions_std    | 0.481    |\n",
            "| rollout/episode_steps  | 179      |\n",
            "| rollout/episodes       | 670      |\n",
            "| rollout/return         | -6.76    |\n",
            "| rollout/return_history | -7.55    |\n",
            "| total/duration         | 322      |\n",
            "| total/episodes         | 670      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 119998   |\n",
            "| total/steps_per_second | 373      |\n",
            "| train/loss_actor       | 0.113    |\n",
            "| train/loss_critic      | 5.18e-06 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.632   |\n",
            "| reference_Q_std        | 0.61     |\n",
            "| reference_action_mean  | 0.0892   |\n",
            "| reference_action_std   | 0.965    |\n",
            "| reference_actor_Q_mean | -0.596   |\n",
            "| reference_actor_Q_std  | 0.602    |\n",
            "| rollout/Q_mean         | -0.389   |\n",
            "| rollout/actions_mean   | 0.0911   |\n",
            "| rollout/actions_std    | 0.488    |\n",
            "| rollout/episode_steps  | 185      |\n",
            "| rollout/episodes       | 675      |\n",
            "| rollout/return         | -6.8     |\n",
            "| rollout/return_history | -7.79    |\n",
            "| total/duration         | 349      |\n",
            "| total/episodes         | 675      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 129998   |\n",
            "| total/steps_per_second | 372      |\n",
            "| train/loss_actor       | 0.234    |\n",
            "| train/loss_critic      | 2.91e-07 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.611   |\n",
            "| reference_Q_std        | 0.6      |\n",
            "| reference_action_mean  | -0.0724  |\n",
            "| reference_action_std   | 0.941    |\n",
            "| reference_actor_Q_mean | -0.575   |\n",
            "| reference_actor_Q_std  | 0.586    |\n",
            "| rollout/Q_mean         | -0.386   |\n",
            "| rollout/actions_mean   | 0.0979   |\n",
            "| rollout/actions_std    | 0.49     |\n",
            "| rollout/episode_steps  | 202      |\n",
            "| rollout/episodes       | 690      |\n",
            "| rollout/return         | -6.92    |\n",
            "| rollout/return_history | -8.7     |\n",
            "| total/duration         | 378      |\n",
            "| total/episodes         | 690      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 139998   |\n",
            "| total/steps_per_second | 371      |\n",
            "| train/loss_actor       | 0.276    |\n",
            "| train/loss_critic      | 1.17e-06 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.598   |\n",
            "| reference_Q_std        | 0.549    |\n",
            "| reference_action_mean  | -0.313   |\n",
            "| reference_action_std   | 0.886    |\n",
            "| reference_actor_Q_mean | -0.562   |\n",
            "| reference_actor_Q_std  | 0.537    |\n",
            "| rollout/Q_mean         | -0.382   |\n",
            "| rollout/actions_mean   | 0.107    |\n",
            "| rollout/actions_std    | 0.495    |\n",
            "| rollout/episode_steps  | 216      |\n",
            "| rollout/episodes       | 694      |\n",
            "| rollout/return         | -7.05    |\n",
            "| rollout/return_history | -9.55    |\n",
            "| total/duration         | 406      |\n",
            "| total/episodes         | 694      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 149998   |\n",
            "| total/steps_per_second | 370      |\n",
            "| train/loss_actor       | 0.32     |\n",
            "| train/loss_critic      | 1.3e-07  |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.627   |\n",
            "| reference_Q_std        | 0.517    |\n",
            "| reference_action_mean  | -0.62    |\n",
            "| reference_action_std   | 0.762    |\n",
            "| reference_actor_Q_mean | -0.598   |\n",
            "| reference_actor_Q_std  | 0.512    |\n",
            "| rollout/Q_mean         | -0.383   |\n",
            "| rollout/actions_mean   | 0.114    |\n",
            "| rollout/actions_std    | 0.485    |\n",
            "| rollout/episode_steps  | 227      |\n",
            "| rollout/episodes       | 703      |\n",
            "| rollout/return         | -7.16    |\n",
            "| rollout/return_history | -10.3    |\n",
            "| total/duration         | 433      |\n",
            "| total/episodes         | 703      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 159998   |\n",
            "| total/steps_per_second | 369      |\n",
            "| train/loss_actor       | 0.392    |\n",
            "| train/loss_critic      | 1.09e-06 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.654   |\n",
            "| reference_Q_std        | 0.493    |\n",
            "| reference_action_mean  | -0.629   |\n",
            "| reference_action_std   | 0.763    |\n",
            "| reference_actor_Q_mean | -0.607   |\n",
            "| reference_actor_Q_std  | 0.476    |\n",
            "| rollout/Q_mean         | -0.386   |\n",
            "| rollout/actions_mean   | 0.122    |\n",
            "| rollout/actions_std    | 0.473    |\n",
            "| rollout/episode_steps  | 233      |\n",
            "| rollout/episodes       | 710      |\n",
            "| rollout/return         | -7.23    |\n",
            "| rollout/return_history | -10.9    |\n",
            "| total/duration         | 461      |\n",
            "| total/episodes         | 710      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 169998   |\n",
            "| total/steps_per_second | 368      |\n",
            "| train/loss_actor       | 0.375    |\n",
            "| train/loss_critic      | 5.74e-06 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.581   |\n",
            "| reference_Q_std        | 0.392    |\n",
            "| reference_action_mean  | -0.226   |\n",
            "| reference_action_std   | 0.88     |\n",
            "| reference_actor_Q_mean | -0.524   |\n",
            "| reference_actor_Q_std  | 0.365    |\n",
            "| rollout/Q_mean         | -0.387   |\n",
            "| rollout/actions_mean   | 0.128    |\n",
            "| rollout/actions_std    | 0.462    |\n",
            "| rollout/episode_steps  | 250      |\n",
            "| rollout/episodes       | 716      |\n",
            "| rollout/return         | -7.42    |\n",
            "| rollout/return_history | -12.3    |\n",
            "| total/duration         | 490      |\n",
            "| total/episodes         | 716      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 179998   |\n",
            "| total/steps_per_second | 367      |\n",
            "| train/loss_actor       | 0.345    |\n",
            "| train/loss_critic      | 5.66e-06 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.593   |\n",
            "| reference_Q_std        | 0.384    |\n",
            "| reference_action_mean  | 0.141    |\n",
            "| reference_action_std   | 0.99     |\n",
            "| reference_actor_Q_mean | -0.538   |\n",
            "| reference_actor_Q_std  | 0.34     |\n",
            "| rollout/Q_mean         | -0.39    |\n",
            "| rollout/actions_mean   | 0.13     |\n",
            "| rollout/actions_std    | 0.452    |\n",
            "| rollout/episode_steps  | 260      |\n",
            "| rollout/episodes       | 729      |\n",
            "| rollout/return         | -7.48    |\n",
            "| rollout/return_history | -12.8    |\n",
            "| total/duration         | 518      |\n",
            "| total/episodes         | 729      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 189998   |\n",
            "| total/steps_per_second | 367      |\n",
            "| train/loss_actor       | 0.232    |\n",
            "| train/loss_critic      | 6.86e-07 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.589   |\n",
            "| reference_Q_std        | 0.317    |\n",
            "| reference_action_mean  | -0.478   |\n",
            "| reference_action_std   | 0.828    |\n",
            "| reference_actor_Q_mean | -0.544   |\n",
            "| reference_actor_Q_std  | 0.274    |\n",
            "| rollout/Q_mean         | -0.393   |\n",
            "| rollout/actions_mean   | 0.132    |\n",
            "| rollout/actions_std    | 0.443    |\n",
            "| rollout/episode_steps  | 270      |\n",
            "| rollout/episodes       | 740      |\n",
            "| rollout/return         | -7.54    |\n",
            "| rollout/return_history | -13.2    |\n",
            "| total/duration         | 546      |\n",
            "| total/episodes         | 740      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 199998   |\n",
            "| total/steps_per_second | 366      |\n",
            "| train/loss_actor       | 0.366    |\n",
            "| train/loss_critic      | 1.25e-06 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.603   |\n",
            "| reference_Q_std        | 0.295    |\n",
            "| reference_action_mean  | -0.64    |\n",
            "| reference_action_std   | 0.766    |\n",
            "| reference_actor_Q_mean | -0.563   |\n",
            "| reference_actor_Q_std  | 0.274    |\n",
            "| rollout/Q_mean         | -0.402   |\n",
            "| rollout/actions_mean   | 0.138    |\n",
            "| rollout/actions_std    | 0.435    |\n",
            "| rollout/episode_steps  | 277      |\n",
            "| rollout/episodes       | 753      |\n",
            "| rollout/return         | -7.66    |\n",
            "| rollout/return_history | -14      |\n",
            "| total/duration         | 575      |\n",
            "| total/episodes         | 753      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 209998   |\n",
            "| total/steps_per_second | 365      |\n",
            "| train/loss_actor       | 0.554    |\n",
            "| train/loss_critic      | 1.71e-07 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.692   |\n",
            "| reference_Q_std        | 0.396    |\n",
            "| reference_action_mean  | 0.141    |\n",
            "| reference_action_std   | 0.99     |\n",
            "| reference_actor_Q_mean | -0.672   |\n",
            "| reference_actor_Q_std  | 0.372    |\n",
            "| rollout/Q_mean         | -0.408   |\n",
            "| rollout/actions_mean   | 0.139    |\n",
            "| rollout/actions_std    | 0.431    |\n",
            "| rollout/episode_steps  | 286      |\n",
            "| rollout/episodes       | 769      |\n",
            "| rollout/return         | -7.72    |\n",
            "| rollout/return_history | -14.3    |\n",
            "| total/duration         | 603      |\n",
            "| total/episodes         | 769      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 219998   |\n",
            "| total/steps_per_second | 365      |\n",
            "| train/loss_actor       | 0.529    |\n",
            "| train/loss_critic      | 0.00169  |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.693   |\n",
            "| reference_Q_std        | 0.362    |\n",
            "| reference_action_mean  | 0.141    |\n",
            "| reference_action_std   | 0.99     |\n",
            "| reference_actor_Q_mean | -0.672   |\n",
            "| reference_actor_Q_std  | 0.334    |\n",
            "| rollout/Q_mean         | -0.406   |\n",
            "| rollout/actions_mean   | 0.135    |\n",
            "| rollout/actions_std    | 0.427    |\n",
            "| rollout/episode_steps  | 294      |\n",
            "| rollout/episodes       | 776      |\n",
            "| rollout/return         | -7.72    |\n",
            "| rollout/return_history | -13.5    |\n",
            "| total/duration         | 632      |\n",
            "| total/episodes         | 776      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 229998   |\n",
            "| total/steps_per_second | 364      |\n",
            "| train/loss_actor       | 0.307    |\n",
            "| train/loss_critic      | 1.15e-07 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.745   |\n",
            "| reference_Q_std        | 0.432    |\n",
            "| reference_action_mean  | 0.141    |\n",
            "| reference_action_std   | 0.99     |\n",
            "| reference_actor_Q_mean | -0.692   |\n",
            "| reference_actor_Q_std  | 0.385    |\n",
            "| rollout/Q_mean         | -0.405   |\n",
            "| rollout/actions_mean   | 0.132    |\n",
            "| rollout/actions_std    | 0.42     |\n",
            "| rollout/episode_steps  | 304      |\n",
            "| rollout/episodes       | 784      |\n",
            "| rollout/return         | -7.71    |\n",
            "| rollout/return_history | -13.3    |\n",
            "| total/duration         | 660      |\n",
            "| total/episodes         | 784      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 239998   |\n",
            "| total/steps_per_second | 364      |\n",
            "| train/loss_actor       | 0.335    |\n",
            "| train/loss_critic      | 2.79e-07 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.73    |\n",
            "| reference_Q_std        | 0.417    |\n",
            "| reference_action_mean  | 0.137    |\n",
            "| reference_action_std   | 0.987    |\n",
            "| reference_actor_Q_mean | -0.696   |\n",
            "| reference_actor_Q_std  | 0.367    |\n",
            "| rollout/Q_mean         | -0.404   |\n",
            "| rollout/actions_mean   | 0.13     |\n",
            "| rollout/actions_std    | 0.413    |\n",
            "| rollout/episode_steps  | 315      |\n",
            "| rollout/episodes       | 791      |\n",
            "| rollout/return         | -7.72    |\n",
            "| rollout/return_history | -13      |\n",
            "| total/duration         | 689      |\n",
            "| total/episodes         | 791      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 249998   |\n",
            "| total/steps_per_second | 363      |\n",
            "| train/loss_actor       | 0.388    |\n",
            "| train/loss_critic      | 9.49e-07 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.778   |\n",
            "| reference_Q_std        | 0.432    |\n",
            "| reference_action_mean  | 0.14     |\n",
            "| reference_action_std   | 0.99     |\n",
            "| reference_actor_Q_mean | -0.763   |\n",
            "| reference_actor_Q_std  | 0.429    |\n",
            "| rollout/Q_mean         | -0.406   |\n",
            "| rollout/actions_mean   | 0.126    |\n",
            "| rollout/actions_std    | 0.406    |\n",
            "| rollout/episode_steps  | 324      |\n",
            "| rollout/episodes       | 803      |\n",
            "| rollout/return         | -7.7     |\n",
            "| rollout/return_history | -11.5    |\n",
            "| total/duration         | 718      |\n",
            "| total/episodes         | 803      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 259998   |\n",
            "| total/steps_per_second | 362      |\n",
            "| train/loss_actor       | 0.625    |\n",
            "| train/loss_critic      | 0.00133  |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.868   |\n",
            "| reference_Q_std        | 0.552    |\n",
            "| reference_action_mean  | 0.132    |\n",
            "| reference_action_std   | 0.983    |\n",
            "| reference_actor_Q_mean | -0.819   |\n",
            "| reference_actor_Q_std  | 0.523    |\n",
            "| rollout/Q_mean         | -0.409   |\n",
            "| rollout/actions_mean   | 0.124    |\n",
            "| rollout/actions_std    | 0.401    |\n",
            "| rollout/episode_steps  | 328      |\n",
            "| rollout/episodes       | 815      |\n",
            "| rollout/return         | -7.69    |\n",
            "| rollout/return_history | -9.89    |\n",
            "| total/duration         | 746      |\n",
            "| total/episodes         | 815      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 269998   |\n",
            "| total/steps_per_second | 362      |\n",
            "| train/loss_actor       | 0.375    |\n",
            "| train/loss_critic      | 1.12e-06 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.795   |\n",
            "| reference_Q_std        | 0.413    |\n",
            "| reference_action_mean  | -0.636   |\n",
            "| reference_action_std   | 0.757    |\n",
            "| reference_actor_Q_mean | -0.742   |\n",
            "| reference_actor_Q_std  | 0.341    |\n",
            "| rollout/Q_mean         | -0.41    |\n",
            "| rollout/actions_mean   | 0.124    |\n",
            "| rollout/actions_std    | 0.395    |\n",
            "| rollout/episode_steps  | 340      |\n",
            "| rollout/episodes       | 822      |\n",
            "| rollout/return         | -7.71    |\n",
            "| rollout/return_history | -9.57    |\n",
            "| total/duration         | 776      |\n",
            "| total/episodes         | 822      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 279998   |\n",
            "| total/steps_per_second | 361      |\n",
            "| train/loss_actor       | 0.557    |\n",
            "| train/loss_critic      | 1.28e-06 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.812   |\n",
            "| reference_Q_std        | 0.423    |\n",
            "| reference_action_mean  | 0.14     |\n",
            "| reference_action_std   | 0.99     |\n",
            "| reference_actor_Q_mean | -0.766   |\n",
            "| reference_actor_Q_std  | 0.376    |\n",
            "| rollout/Q_mean         | -0.416   |\n",
            "| rollout/actions_mean   | 0.126    |\n",
            "| rollout/actions_std    | 0.39     |\n",
            "| rollout/episode_steps  | 347      |\n",
            "| rollout/episodes       | 834      |\n",
            "| rollout/return         | -7.78    |\n",
            "| rollout/return_history | -9.92    |\n",
            "| total/duration         | 804      |\n",
            "| total/episodes         | 834      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 289998   |\n",
            "| total/steps_per_second | 361      |\n",
            "| train/loss_actor       | 0.333    |\n",
            "| train/loss_critic      | 1.51e-06 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.732   |\n",
            "| reference_Q_std        | 0.377    |\n",
            "| reference_action_mean  | 0.14     |\n",
            "| reference_action_std   | 0.989    |\n",
            "| reference_actor_Q_mean | -0.64    |\n",
            "| reference_actor_Q_std  | 0.32     |\n",
            "| rollout/Q_mean         | -0.417   |\n",
            "| rollout/actions_mean   | 0.125    |\n",
            "| rollout/actions_std    | 0.386    |\n",
            "| rollout/episode_steps  | 355      |\n",
            "| rollout/episodes       | 845      |\n",
            "| rollout/return         | -7.78    |\n",
            "| rollout/return_history | -9.18    |\n",
            "| total/duration         | 833      |\n",
            "| total/episodes         | 845      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 299998   |\n",
            "| total/steps_per_second | 360      |\n",
            "| train/loss_actor       | 0.299    |\n",
            "| train/loss_critic      | 1.11e-05 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.605   |\n",
            "| reference_Q_std        | 0.339    |\n",
            "| reference_action_mean  | 0.141    |\n",
            "| reference_action_std   | 0.99     |\n",
            "| reference_actor_Q_mean | -0.551   |\n",
            "| reference_actor_Q_std  | 0.332    |\n",
            "| rollout/Q_mean         | -0.415   |\n",
            "| rollout/actions_mean   | 0.122    |\n",
            "| rollout/actions_std    | 0.389    |\n",
            "| rollout/episode_steps  | 363      |\n",
            "| rollout/episodes       | 852      |\n",
            "| rollout/return         | -7.77    |\n",
            "| rollout/return_history | -8.64    |\n",
            "| total/duration         | 861      |\n",
            "| total/episodes         | 852      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 309998   |\n",
            "| total/steps_per_second | 360      |\n",
            "| train/loss_actor       | 0.293    |\n",
            "| train/loss_critic      | 2.81e-07 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.627   |\n",
            "| reference_Q_std        | 0.348    |\n",
            "| reference_action_mean  | 0.141    |\n",
            "| reference_action_std   | 0.99     |\n",
            "| reference_actor_Q_mean | -0.617   |\n",
            "| reference_actor_Q_std  | 0.332    |\n",
            "| rollout/Q_mean         | -0.413   |\n",
            "| rollout/actions_mean   | 0.117    |\n",
            "| rollout/actions_std    | 0.396    |\n",
            "| rollout/episode_steps  | 370      |\n",
            "| rollout/episodes       | 865      |\n",
            "| rollout/return         | -7.75    |\n",
            "| rollout/return_history | -7.91    |\n",
            "| total/duration         | 890      |\n",
            "| total/episodes         | 865      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 319998   |\n",
            "| total/steps_per_second | 360      |\n",
            "| train/loss_actor       | 0.335    |\n",
            "| train/loss_critic      | 2.22e-06 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.822   |\n",
            "| reference_Q_std        | 0.414    |\n",
            "| reference_action_mean  | 0.141    |\n",
            "| reference_action_std   | 0.99     |\n",
            "| reference_actor_Q_mean | -0.82    |\n",
            "| reference_actor_Q_std  | 0.417    |\n",
            "| rollout/Q_mean         | -0.413   |\n",
            "| rollout/actions_mean   | 0.106    |\n",
            "| rollout/actions_std    | 0.407    |\n",
            "| rollout/episode_steps  | 376      |\n",
            "| rollout/episodes       | 877      |\n",
            "| rollout/return         | -7.84    |\n",
            "| rollout/return_history | -8.74    |\n",
            "| total/duration         | 918      |\n",
            "| total/episodes         | 877      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 329998   |\n",
            "| total/steps_per_second | 359      |\n",
            "| train/loss_actor       | 0.357    |\n",
            "| train/loss_critic      | 8.13e-06 |\n",
            "-------------------------------------\n",
            "\n",
            "-------------------------------------\n",
            "| reference_Q_mean       | -0.854   |\n",
            "| reference_Q_std        | 0.416    |\n",
            "| reference_action_mean  | 0.141    |\n",
            "| reference_action_std   | 0.99     |\n",
            "| reference_actor_Q_mean | -0.838   |\n",
            "| reference_actor_Q_std  | 0.41     |\n",
            "| rollout/Q_mean         | -0.413   |\n",
            "| rollout/actions_mean   | 0.0988   |\n",
            "| rollout/actions_std    | 0.416    |\n",
            "| rollout/episode_steps  | 384      |\n",
            "| rollout/episodes       | 885      |\n",
            "| rollout/return         | -7.87    |\n",
            "| rollout/return_history | -9.11    |\n",
            "| total/duration         | 947      |\n",
            "| total/episodes         | 885      |\n",
            "| total/epochs           | 1        |\n",
            "| total/steps            | 339998   |\n",
            "| total/steps_per_second | 359      |\n",
            "| train/loss_actor       | 0.276    |\n",
            "| train/loss_critic      | 1.51e-05 |\n",
            "-------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}